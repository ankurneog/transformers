{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "device=None\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif (hasattr(torch, \"hpu\") and torch.hpu.is_available()):\n",
    "    device = 'hpu'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps' # mac\n",
    "else :\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device  : {device}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set source : Hugging Face \n",
    "# https://huggingface.co/datasets/leonweber/teaching_motivational_quotes/tree/main\n",
    "\n",
    "torch.manual_seed(12131306)\n",
    "batch_size = 16\n",
    "text = None\n",
    "vocab_size = 1000\n",
    "\n",
    "with open('data/motivational_quotes.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(torch.nn.Module): # attention head\n",
    "    def __init__(self,head_size, embedding_dimension,context_length):\n",
    "        super.__init__()\n",
    "        \"\"\"one head of the self-attention block\n",
    "        its a nxm matrics and hence we can consider it as a linear block with bias=False\n",
    "        here n : input embedding size and m : head size (which is typically kept as embedding dimension\n",
    "        divided by the number of heads, this is done so that in the end when we concatenate the output\n",
    "        of the attention heads they become equal to the embedding dimension \n",
    "        typically for models such as bert and gpt the embedding dimension used is 768, and the number of heads \n",
    "        is 12 , hence the number of colums for the weights should be 768/12 =64 , hence the dimension is \n",
    "        768x64 )\"\"\"\n",
    "        self.head_size = head_size\n",
    "\n",
    "        #weights for the Q,K,V vectors\n",
    "        self.Qw = torch.nn.Linear(embedding_dimension, self.head_size, bias=False)\n",
    "        self.Qw = torch.nn.Linear(embedding_dimension, self.head_size, bias=False)\n",
    "        self.Qw = torch.nn.Linear(embedding_dimension, self.head_size, bias=False)\n",
    "\n",
    "        \"\"\"Next we create our tril tensor to mask the output for auto regressive training,\n",
    "        we will use the register_buffer utiltiy so that this tensor is not considered part of \n",
    "        the model parameters but rather considered as a model state.\n",
    "        we name the buffer as tril\"\"\"\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_length, context_length)))\n",
    "\n",
    "        \"\"\"Next define a drop out layer\n",
    "        The main purpose of incorporating dropout in the attention block is to mitigate overfitting,\n",
    "        especially in models with a large number of parameters. By randomly dropping units during training, \n",
    "        the model learns to be more robust and less reliant on specific features.\"\"\"\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.2)  #default value is 0.5, we set it to 0.2 \n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"To get the q,k,v vectors we should matmul the input with the wieghts of the q,k,v vectors\n",
    "        the input dimensions is B: batch size, T: context_length, D : embedding dimension\n",
    "        hence to get proper output we should have the output from each attention head as \n",
    "        B,T and Head dimension i.e (D/num_attention_heads)\"\"\"\n",
    "        batch_size,token_length,embedding_dimension = x.shape\n",
    "        \"\"\"creating the q,k,v vectors :\n",
    "            k = self.key(x):\n",
    "            This line applies the linear transformation defined by self.key to the input tensor x.\n",
    "            The input x is expected to have a shape of (batch_size, sequence_length, n_embd), where:\n",
    "            batch_size is the number of samples in the batch.\n",
    "            sequence_length is the length of each input sequence.\n",
    "            n_embd is the dimensionality of the input embeddings.\n",
    "            The linear layer projects each input vector of size n_embd to a vector of size head_size, \n",
    "            resulting in a tensor k with shape (batch_size, sequence_length, head_size).\n",
    "            This tensor k represents the key vectors for the attention mechanism.\"\"\"\n",
    "        Q = self.Qw(x) # project x to Qw , the output is of dimension batch_size,context_length, head_size\n",
    "        K = self.Kw(x) # batch_size,context_length,head_size\n",
    "        V = self.Vw(x) # batch_size,context_length,head_size\n",
    "\n",
    "        \"\"\"Now compute the attention score : the scaled dot product attention formulat is :\n",
    "        Attention (Q,K,V) = softmax(((Q@Kt/sqrt(head_size)))@V\n",
    "        1. compute Q@Kt we transpose the Kt in the last two dimension\n",
    "            this transpose can be achieve by either k.transpose(-2,-1)  - swap the last two dimensions\n",
    "            or we can also use permute , k.permute(0,3)\"\"\"\n",
    "        out = Q @ K.transpose(-2,-1) # dot product of Q and K , (batch_size,context_length,head_size) @ (batch_size, head_size,context_length) --> (batch_size,context_length,context_length)\n",
    "        scaling = K.shape(-1)**0.5 # square root of the head dimension of K\n",
    "        out = out/scaling\n",
    "        out = torch.softmax(out) # softmax of the output\n",
    "        out = out @ V # (batch_size,context_length, context_length) @ (batch_size, context_legth, head_size)--> (batch_size,context_length,head_size)\n",
    "        return out \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Multi-headed attention , We need to have multi-headed attention , for this to preserve the dimensionality this number is chosen so that its divisible by the embedding dimension\n",
    "for the original attention paper the embedding dimension is 764 , the numb attention heads was chosen as 12 , hence the size of each head is 764/12 == 64\n",
    "thus at the output of the attention block we can concatenate all the attention heads to arrive at the output dimension 764 (12*64)\"\"\"\n",
    "class MultiHeadedAttention(torch.nn.Module):\n",
    "    def __init__(self, num_head, head_size,embedding_dimension,context_length):\n",
    "        super.__init__()\n",
    "        self.num_head = num_head\n",
    "        self.heads = torch.nn.ModuleList([Head(head_size,embedding_dimension,context_length) for _ in range(num_head)])\n",
    "        self.projection = torch.Linear(head_size * num_head, embedding_dimension)\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"concatenate all the attention heads, project it to a Linear module with trainable prameters and add drop out\"\"\"\n",
    "        out = torch.concat([h(x) for h in self.heads],dim=-1) # concatenate the last dimension , i.e the head_size\n",
    "        out = self.projection(out)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The output of the attention block is fed to a Fully connected feed forward network with relu activation\"\"\"\n",
    "class FeedForward(torch.nn.Module):\n",
    "    def __init__(self,embedding_dimension):\n",
    "        super.__init__()\n",
    "        self.fc = torch.nn.Sequential(torch.nn.Linear(embedding_dimension,4 * embedding_dimension),\n",
    "                                      torch.nn.ReLU(),\n",
    "                                      torch.nn.Linear(4 * embedding_dimension, embedding_dimension),\n",
    "                                      torch.nn.Dropout(0.2)\n",
    "                                    )\n",
    "    def forward(self,x):\n",
    "        return self.fc(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Stiching the multi-headed attention to a Feedforward and adding layer norm , also adding the residual connections - The Transformers block\"\"\"\n",
    "class TransformerBlock(torch.nn.Module):\n",
    "    def __init__(self,num_head,head_size,embedding_dimension,context_length):\n",
    "        \"\"\"We will try to create a decoder block, the components of the decoder block are \n",
    "        1. The inputs are the positional embedding vectors of each token x\n",
    "        2. The next layer is a Layer Norm \n",
    "        3. Followed by SDPA (Scaled Dot Product Attention)\n",
    "        4. Drop Out (Our MultiHeadedAttention Has the dropout included )\n",
    "        5. Layer Normalization \n",
    "        6. FeedForward Network\n",
    "        7. Drop Out \n",
    "        8. In addition there are some residual connections\n",
    "        \"\"\"\n",
    "        super.__init__()\n",
    "        self.spda = MultiHeadedAttention(num_head, head_size, embedding_dimension, context_length)\n",
    "        self.ffwdnet = FeedForward(embedding_dimension)\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(embedding_dimension)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(embedding_dimension)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.sdpa(self.layer_norm_1(x))\n",
    "        out = x + self.ffwdnet(self.layer_norm_2())\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Making the GPT2 Decoder Block \n",
    "The decoder Block consists of the following \n",
    "1. Tokenizer (eg: BPE, sentence-piece, GPT uses BPE aka Byte pair encoding)\n",
    "2. Word Embedding (word2vec)\n",
    "3. Positional Embedding \n",
    "4. 12 * Transformer Blocks\n",
    "5. Layer Normalization\n",
    "6. Feed Forward\"\"\"\n",
    "\n",
    "\"\"\"The output of the GPT2Decoder can have many other heads such as summarization head, causal language modelling, translation head etc.\n",
    "1. Classification Head\n",
    "This head is used for tasks where the model needs to classify the input into predefined categories. It typically consists of one or more fully connected layers followed by a softmax activation function to output probabilities for each class.\n",
    "2. Regression Head\n",
    "Used for tasks that require predicting continuous values, such as predicting prices or coordinates. This head usually consists of fully connected layers without an activation function at the output layer (or with a linear activation).\n",
    "3. Sequence-to-Sequence Head\n",
    "This head is suitable for tasks like translation or summarization, where the model generates a sequence of outputs based on the input sequence. It often involves a decoder structure that can process the output from the GPT-2 block.\n",
    "4. Token Classification Head\n",
    "Used for tasks like Named Entity Recognition (NER) or Part-of-Speech (POS) tagging, where each token in the input sequence needs to be classified. This head typically outputs a classification for each token.\n",
    "5. Multi-Task Head\n",
    "This head can handle multiple tasks simultaneously, combining different types of outputs (e.g., classification and regression) from a single model. It may involve multiple branches in the architecture.\n",
    "6. Attention Heads\n",
    "Additional attention heads can be added for specialized attention mechanisms, allowing the model to focus on different aspects of the input data. This is particularly useful in multi-head attention setups.\n",
    "7. Mixture of Experts Head\n",
    "This head leverages multiple expert models, activating only a subset of them for each input, which can improve efficiency and performance on diverse tasks.\n",
    "8. Contrastive Learning Head\n",
    "Used in self-supervised learning scenarios, this head is designed to maximize the similarity between positive pairs and minimize it between negative pairs, often implemented using a projection head followed by a contrastive loss.\n",
    "9. Generative Head\n",
    "For tasks involving text generation, this head can be designed to produce sequences based on the learned representations, often using techniques like beam search or sampling.\n",
    "\"\"\"\n",
    "\n",
    "class GPTLanguageModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size, context_length, embedding_dimension,num_transformer_blocks,num_attention_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size=vocab_size\n",
    "        self.context_length = context_length\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.num_attention_heads = num_attention_heads \n",
    "        self.head_dimension = embedding_dimension/num_attention_heads\n",
    "\n",
    "        \"\"\" GPT2 Pipeline\n",
    "        [Tokenizer]->[Word Embeddings]->[Postional Embedding]->[TrasformerBlock1: [[Layer Norm]->[Self Attention Block x12]->[Drop Out]->[Layer Norm]->[FC layers]->[Drop Out]].....\n",
    "        [TransformerBlock12: [[Layer Norm]->[Self Attention Block x12]->[Drop Out]->[Layer Norm]->[FC layers]->[Drop Out] ] ->[Layer Normalization]->[GPT2 Workload Head/ Linear Layer]\"\"\"\n",
    "\n",
    "        #https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.token_embedding_table = torch.nn.Embedding(self.vocab_size, self.embedding_dimension) #Word Embedding\n",
    "        self.position_embedding_table = torch.nn.Embedding(self.context_length, self.embedding_dimension) # Positional Embedding\n",
    "        self.self_attention_blocks = torch.nn.Sequential(*[TransformerBlock(self.num_attention_heads,self.head_dimension,self.embedding_dimension,self.context_length) for _ in range(num_transformer_blocks)]) # Self Attention\n",
    "        self.ln_f = torch.nn.LayerNorm(self.embedding_dimension)   # Layer Norm after self attention block \n",
    "        self.lm_head = torch.nn.Linear(self.embedding_dimension, self.vocab_size) # GPT2 Workload Head\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, torch.nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_tokens, targets=None):\n",
    "        batch_size, context_length = input_tokens.shape # Batch size x Context_length (max tokens supported)\n",
    "\n",
    "        tok_emb = self.token_embedding_table(input_tokens) # Dimension : batch_size, context_length, embedding_dimension\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # Dimension : token_size x embedding_dimension\n",
    "        x = tok_emb + pos_emb # Dimension : batch_size, context_length, embedding_dimension\n",
    "        x = self.self_attention_blocks(x) # Dimension : batch_size, context_length, embedding_dimension\n",
    "        x = self.ln_f(x) # Dimension : batch_size, context_length, embedding_dimension \n",
    "        logits = self.lm_head(x) # Logits -> The raw results , Dimension : batch_size, context_length, vocab_size\n",
    "\n",
    "        \"\"\"  We need to reshape the logits since the cross entropy loss only accepts m x n format : \n",
    "             original shape of logits was (batch_size, context_length, vocab_size).\n",
    "             loss functions, such as cross-entropy loss, expect the logits to have a shape of (num_samples, num_classes), where : \n",
    "             num_samples is the total number of samples (in this case, batch_size * context_length) and num_classes is the number of classes (in this case, vocab_size).\n",
    "             After reshaping, the new shape becomes (batch_size * context_length, vocab_size). This effectively flattens the first two dimensions of logits into a single dimension.\n",
    "        \"\"\"\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            batch_size, context_length, vocab_size = logits.shape\n",
    "            logits = logits.view(batch_size*context_length, vocab_size) # reshape to 2D tensor\n",
    "            targets = targets.view(batch_size*context_length) # 1D tensor , with the next token \n",
    "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss # return logits in Dimension : batch_size*context_length, vocab_size, \n",
    "\n",
    "    \"\"\"The generator :\n",
    "        causal next word prediction \n",
    "        eg : THe world is a ......\n",
    "        prediction : The world is a mystery in itself\n",
    "        generate_token() -> this is our main generator, Arguments : \n",
    "        input token : The input given by the user , Dimension : batch_size x context_length\n",
    "        max_new_tokens : maximum length of the generated text/tokens\n",
    "    \"\"\"\n",
    "    def generate_tokens(self, input, max_size_of_generated_tokens): \n",
    "        \n",
    "        for _ in range(max_size_of_generated_tokens):\n",
    "            # Take only the tokens till the max context length \n",
    "           \n",
    "            input_tokens = input[:, -self.context_length:] # Dimension : batch_size x context_length\n",
    "          \n",
    "            # get the raw data of size vocab_size\n",
    "            logits, loss = self(input_tokens) # Dimension logits : batch_size x context_length x vocab_size\n",
    "           \n",
    "            # Take the slice of the last token , the resulting \n",
    "            logits = logits[:, -1, :]  # The resultant tensor will be of size batch_size x vocab_size , data only of the last tokens in those dimension\n",
    "            \n",
    "            # Now apply Softmax along the vocab_size dimension to get the probabilites \n",
    "           \n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1) # sofmax along column, so that rows have probabilty of 1\n",
    "            # use torch.multinomial to select decrete values from probability distribution , we would like to select one index from a row ( of batches)\n",
    "            # hence we set the num_samples to 1.\n",
    "            #https://pytorch.org/docs/stable/generated/torch.multinomial.html\n",
    "            # This is our next token \n",
    "            input_next = torch.multinomial(probs, num_samples=1)  # Dimension : batch_size x 1\n",
    "            # append this to the next input token \n",
    "            input = torch.cat((input, input_next), dim=1) # \n",
    "        return input\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
